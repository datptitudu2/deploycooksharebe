{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üç≥ CookBot Training Notebook\n",
        "\n",
        "Train chatbot AI ri√™ng cho CookShare - **KH√îNG C·∫¶N API KEY!**\n",
        "\n",
        "## M·ª•c ti√™u:\n",
        "1. Fine-tune model GPT-2 v·ªõi dataset m√≥n ƒÉn Vi·ªát Nam\n",
        "2. Upload model l√™n Hugging Face (FREE)\n",
        "3. S·ª≠ d·ª•ng mi·ªÖn ph√≠, ho√†n to√†n ƒë·ªôc l·∫≠p\n",
        "\n",
        "## Y√™u c·∫ßu:\n",
        "- Google Colab (FREE GPU)\n",
        "- T√†i kho·∫£n Hugging Face (FREE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. C√†i ƒë·∫∑t th∆∞ vi·ªán\n",
        "!pip install transformers datasets accelerate peft -q\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Upload dataset t·ª´ local\n",
        "from google.colab import files\n",
        "print(\"Upload file: dataset_cookbot.jsonl v√† dataset_cookbot_part2.jsonl\")\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Load v√† chu·∫©n b·ªã data\n",
        "import json\n",
        "import os\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_jsonl_files(files):\n",
        "    data = []\n",
        "    for f in files:\n",
        "        if os.path.exists(f):\n",
        "            with open(f, 'r', encoding='utf-8') as file:\n",
        "                for line in file:\n",
        "                    if line.strip():\n",
        "                        data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "def format_conversation(example):\n",
        "    messages = example['messages']\n",
        "    text = \"\"\n",
        "    for msg in messages:\n",
        "        role, content = msg['role'], msg['content']\n",
        "        if role == 'system':\n",
        "            text += f\"<|system|>{content}</s>\\n\"\n",
        "        elif role == 'user':\n",
        "            text += f\"<|user|>{content}</s>\\n\"\n",
        "        elif role == 'assistant':\n",
        "            text += f\"<|assistant|>{content}</s>\\n\"\n",
        "    return text\n",
        "\n",
        "# Load data\n",
        "files_list = ['dataset_cookbot.jsonl', 'dataset_cookbot_part2.jsonl']\n",
        "data = load_jsonl_files(files_list)\n",
        "formatted = [{'text': format_conversation(d)} for d in data]\n",
        "dataset = Dataset.from_list(formatted)\n",
        "print(f\"Loaded {len(dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Load model GPT-2\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['<|system|>', '<|user|>', '<|assistant|>']})\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"c_attn\", \"c_proj\"], lora_dropout=0.1, task_type=\"CAUSAL_LM\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Tokenize v√† train\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, max_length=512, padding='max_length')\n",
        "\n",
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
        "split = tokenized.train_test_split(test_size=0.1)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./cookbot-model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=split['train'],\n",
        "    eval_dataset=split['test'],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# TRAIN!\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Test model\n",
        "def generate(prompt):\n",
        "    inp = f\"<|system|>B·∫°n l√† CookBot - AI t∆∞ v·∫•n m√≥n ƒÉn.</s>\\n<|user|>{prompt}</s>\\n<|assistant|>\"\n",
        "    inputs = tokenizer(inp, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.decode(out[0], skip_special_tokens=False)\n",
        "    if '<|assistant|>' in response:\n",
        "        response = response.split('<|assistant|>')[-1].split('</s>')[0]\n",
        "    return response.strip()\n",
        "\n",
        "# Test\n",
        "for q in [\"Xin ch√†o\", \"H√¥m nay ƒÉn g√¨\", \"T√¥i mu·ªën gi·∫£m c√¢n\"]:\n",
        "    print(f\"User: {q}\")\n",
        "    print(f\"CookBot: {generate(q)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Upload l√™n Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()  # Nh·∫≠p token t·ª´ https://huggingface.co/settings/tokens\n",
        "\n",
        "# Merge v√† save\n",
        "merged = model.merge_and_unload()\n",
        "merged.save_pretrained(\"./cookbot-final\")\n",
        "tokenizer.save_pretrained(\"./cookbot-final\")\n",
        "\n",
        "# Push to hub - Username: uduptit\n",
        "HF_USER = \"uduptit\"  # <-- ƒê√£ thay ƒë·ªïi\n",
        "merged.push_to_hub(f\"{HF_USER}/cookbot-vietnamese\")\n",
        "tokenizer.push_to_hub(f\"{HF_USER}/cookbot-vietnamese\")\n",
        "print(f\"Done! Model: https://huggingface.co/{HF_USER}/cookbot-vietnamese\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
