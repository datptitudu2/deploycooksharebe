# CookBot Model Server - Deploy trÃªn Render

Server Ä‘á»ƒ serve model GPT-2 Ä‘Ã£ fine-tune `uduptit/cookbot-vietnamese` trÃªn Render free tier.

## ğŸš€ Deploy trÃªn Render (Tá»« cÃ¹ng GitHub repo)

### BÆ°á»›c 1: Äáº£m báº£o code Ä‘Ã£ push lÃªn GitHub

```bash
cd CookShare
git add backend/model_server/
git commit -m "Add model server for Render"
git push origin main
```

### BÆ°á»›c 2: Deploy trÃªn Render Dashboard

1. **VÃ o Render Dashboard:**
   - https://dashboard.render.com
   - Click "New" â†’ "Web Service"

2. **Connect GitHub:**
   - Connect GitHub repo (cÃ¹ng repo vá»›i backend)
   - Chá»n repo vÃ  branch (main)

3. **Cáº¥u hÃ¬nh Service:**
   ```
   Name: cookbot-model-server
   Environment: Python 3
   Root Directory: backend/model_server  â† QUAN TRá»ŒNG!
   Build Command: pip install -r requirements.txt
   Start Command: python app.py
   Plan: Free
   ```

4. **Environment Variables (náº¿u cáº§n):**
   - `HF_TOKEN`: Token Hugging Face (náº¿u model private)
   - `PORT`: Tá»± Ä‘á»™ng set bá»Ÿi Render

5. **Click "Create Web Service"**

### BÆ°á»›c 3: Chá» deploy

- **Build time:** 5-10 phÃºt (cÃ i dependencies)
- **First request:** 10-30 giÃ¢y (load model láº§n Ä‘áº§u)
- **URL:** `https://cookbot-model-server.onrender.com`

## ğŸ“ API Endpoints

### 1. Health Check
```bash
GET /health
```

Response:
```json
{
  "status": "ok",
  "model_loaded": true,
  "memory_usage": "250.50MB"
}
```

### 2. Generate Response
```bash
POST /predict
Content-Type: application/json

{
  "prompt": "HÃ´m nay Äƒn gÃ¬?",
  "max_length": 200,
  "temperature": 0.7
}
```

Response:
```json
{
  "response": "Äá»ƒ gá»£i Ã½ mÃ³n ngon cho báº¡n, cho mÃ¬nh biáº¿t: Báº¡n thÃ­ch Äƒn máº·n hay nháº¹ nhÃ ng?",
  "generation_time": "3.45s",
  "model": "uduptit/cookbot-vietnamese"
}
```

## âš™ï¸ Tá»‘i Æ°u cho Render Free Tier

### Memory Optimization:
- âœ… DÃ¹ng `torch.float16` thay vÃ¬ `float32` (giáº£m 50% RAM)
- âœ… `low_cpu_mem_usage=True` khi load model
- âœ… `gc.collect()` sau má»—i request
- âœ… Lazy loading (chá»‰ load khi cáº§n)

### CPU Optimization:
- âœ… `torch.no_grad()` khi inference
- âœ… Batch size = 1
- âœ… Max length = 200 tokens

## ğŸ”§ Local Testing

```bash
# CÃ i Ä‘áº·t dependencies
cd backend/model_server
pip install -r requirements.txt

# Cháº¡y server
python app.py

# Test (terminal khÃ¡c)
curl http://localhost:5000/health
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Xin chÃ o"}'
```

## ğŸ”— Integrate vá»›i Backend

Sau khi deploy xong, cáº­p nháº­t `.env` cá»§a backend:

```env
USE_MODEL_SERVER=true
MODEL_SERVER_URL=https://cookbot-model-server.onrender.com
```

## ğŸ“Š Performance

- **Model load time:** 10-30 giÃ¢y (láº§n Ä‘áº§u)
- **Inference time:** 3-10 giÃ¢y/request (tÃ¹y Ä‘á»™ dÃ i)
- **RAM usage:** ~250-400MB (vá»›i float16)
- **Cold start:** 10-30 giÃ¢y

## âš ï¸ LÆ°u Ã½

1. **Render free tier cÃ³ thá»ƒ sleep sau 15 phÃºt idle**
   - Request Ä‘áº§u tiÃªn sau khi sleep sáº½ cháº­m (cold start)

2. **0.1 CPU ráº¥t cháº­m**
   - Inference cÃ³ thá»ƒ máº¥t 5-10 giÃ¢y
   - NÃªn set timeout cao hÆ¡n á»Ÿ client

3. **512MB RAM ráº¥t Ã­t**
   - Chá»‰ Ä‘á»§ cho GPT-2 Small vá»›i quantization
   - CÃ³ thá»ƒ OOM náº¿u cÃ³ nhiá»u requests cÃ¹ng lÃºc

## ğŸ”— Integrate vá»›i Backend

Cáº­p nháº­t `chatbotSelfHostedController.js`:

```javascript
// ThÃªm option Ä‘á»ƒ dÃ¹ng model server
const USE_MODEL_SERVER = process.env.USE_MODEL_SERVER === 'true';
const MODEL_SERVER_URL = process.env.MODEL_SERVER_URL || 'http://localhost:5000';

async function callModelServer(userMessage) {
  const response = await fetch(`${MODEL_SERVER_URL}/predict`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ prompt: userMessage })
  });
  const data = await response.json();
  return data.response;
}
```

## ğŸ“ Giáº£i thÃ­ch cho GiÃ¡o viÃªn

"Em Ä‘Ã£ deploy model Ä‘Ã£ fine-tune lÃªn Render free tier. Model server cháº¡y Ä‘á»™c láº­p, cÃ³ thá»ƒ gá»i API Ä‘á»ƒ generate response. Tuy nhiÃªn, do Render free tier chá»‰ cÃ³ 512MB RAM vÃ  0.1 CPU nÃªn inference hÆ¡i cháº­m (5-10 giÃ¢y). VÃ¬ váº­y, em váº«n dÃ¹ng Groq API cho production vÃ¬ nhanh hÆ¡n vÃ  free."

