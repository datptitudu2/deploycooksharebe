"""
Flask app ƒë·ªÉ serve model GPT-2 ƒë√£ fine-tune tr√™n Render
T·ªëi ∆∞u cho 512MB RAM v√† 0.1 CPU
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
import gc
import os
import time

app = Flask(__name__)
CORS(app)

# Global variables
model = None
tokenizer = None
model_loaded = False

def load_model():
    """Lazy load model - ch·ªâ load khi c·∫ßn"""
    global model, tokenizer, model_loaded
    
    if model_loaded:
        return model, tokenizer
    
    print("üîÑ ƒêang load model...")
    start_time = time.time()
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        model_name = "uduptit/cookbot-vietnamese"
        
        # Load tokenizer tr∆∞·ªõc
        print("üìñ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token = tokenizer.eos_token
        
        # Load model v·ªõi quantization ƒë·ªÉ ti·∫øt ki·ªám RAM
        print("üì¶ Loading model v·ªõi quantization...")
        # Force CPU cho Render free tier (kh√¥ng c√≥ GPU)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # Gi·∫£m 50% RAM
            device_map="cpu",  # Force CPU (Render free tier kh√¥ng c√≥ GPU)
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # ƒê·∫£m b·∫£o model tr√™n CPU
        model = model.cpu()
        
        # Set eval mode
        model.eval()
        
        # Clear cache
        gc.collect()
        # Kh√¥ng c·∫ßn empty CUDA cache v√¨ d√πng CPU
        
        model_loaded = True
        load_time = time.time() - start_time
        print(f"‚úÖ Model loaded trong {load_time:.2f}s")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"‚ùå L·ªói load model: {e}")
        raise

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'ok',
        'model_loaded': model_loaded,
        'memory_usage': 'CPU mode (Render free tier)'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Generate response t·ª´ model"""
    global model, tokenizer
    
    try:
        data = request.json
        prompt = data.get('prompt', '')
        max_length = data.get('max_length', 200)
        temperature = data.get('temperature', 0.7)
        
        if not prompt:
            return jsonify({'error': 'Prompt kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'}), 400
        
        # Load model n·∫øu ch∆∞a load
        if not model_loaded:
            model, tokenizer = load_model()
        
        # Format prompt theo format c·ªßa training data
        formatted_prompt = f"<|system|>B·∫°n l√† CookBot - AI t∆∞ v·∫•n m√≥n ƒÉn c·ªßa CookShare. Tr·∫£ l·ªùi th√¢n thi·ªán, g·ª£i √Ω m√≥n ƒÉn khi ƒë∆∞·ª£c h·ªèi.</s>\n<|user|>{prompt}</s>\n<|assistant|>"
        
        # Tokenize
        inputs = tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=256)
        
        # Move to same device as model
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=temperature,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.2,
            )
        
        # Decode
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract ch·ªâ ph·∫ßn assistant response
        if "<|assistant|>" in response:
            response = response.split("<|assistant|>")[-1].strip()
        
        generation_time = time.time() - start_time
        
        # Clear cache sau m·ªói request (quan tr·ªçng cho free tier)
        gc.collect()
        # Kh√¥ng c·∫ßn empty CUDA cache v√¨ d√πng CPU
        
        return jsonify({
            'response': response,
            'generation_time': f"{generation_time:.2f}s",
            'model': 'uduptit/cookbot-vietnamese'
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/', methods=['GET'])
def index():
    """Root endpoint"""
    return jsonify({
        'message': 'CookBot Model Server',
        'model': 'uduptit/cookbot-vietnamese',
        'status': 'running',
        'endpoints': {
            '/health': 'Health check',
            '/predict': 'Generate response (POST)'
        }
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)

